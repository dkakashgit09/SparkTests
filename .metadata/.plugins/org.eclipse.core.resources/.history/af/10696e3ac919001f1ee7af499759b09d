package com.sparkapp.service;

import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.functions.row_number;

import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Properties;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.expressions.WindowSpec;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import org.bson.Document;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import com.mongodb.spark.MongoSpark;
import com.mongodb.spark.config.WriteConfig;

import scala.Tuple2;



@Service
public class SparkService 
{
	@Value("${spring.datasource.url}")
	private String mysqlUrl;

    @Value("${spring.datasource.username}")
    private String mysqlUsername;

    @Value("${spring.datasource.password}")
    private String mysqlPassword;

    @Value("${spring.datasource.driver-class-name}")
    private String mysqlDriver;
    
    @Value("${spring.data.mongodb.uri}")
    private String mongodbUri;
    
    @Value("${spring.data.mongodb.database}")
    private String mongodbname;
    
    @Value("${cassandra.host}")
    private String cassandraHost;

    @Value("${cassandra.port}")
    private int cassandraPort;

    @Value("${cassandra.keyspace}")
    private String cassandraKeyspace;
    
    private String destSqlUrl = "jdbc:mysql://localhost:3306/dkakash";
    
    private String destMongoUri = "mongodb://localhost:27017/MongoToMongo";
    private String destMongodbName = "MongoToMongo";
    	
    public String renameColumnInMySql()
    {
        try(SparkSession session = SparkSession.builder().appName("colrename").master("local").getOrCreate())
        {
        	String tableName = "annual_reports";
            String tempTableName = tableName + "_temp";

        	Properties connectionProperties = new Properties();
            connectionProperties.put("user", mysqlUsername);
            connectionProperties.put("password", mysqlPassword);
            
        	//CSV files to DataFrame
    		Dataset<Row> data = session.read().jdbc(mysqlUrl, tableName, connectionProperties);
    		data.show();
    		Dataset<Row> updatedData = data.withColumnRenamed("value", "values");
    		updatedData.show();
    		
            updatedData.write().format("jdbc").mode(SaveMode.Overwrite).option("url", mysqlUrl)
							.option("dbtable", tableName)
							.option("user", mysqlUsername)
							.option("password", mysqlPassword)
							.save();
            

            // Define a temporary table name//
            // Write the renamed DataFrame to a temporary table in MySQL
            updatedData.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);
            // Read back from the temporary table (this ensures the schema is updated in MySQL)
            Dataset<Row> finalDF = session.read()
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);

            // Write the final DataFrame back to the original table, preserving the data
            finalDF.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tableName, connectionProperties);

            session.stop();
            return "success";
        } catch (Exception e) {
            e.printStackTrace();
            return "Spark";
        }
        
    }
    
    public String modifyDataInMySql()
    {
        try(SparkSession session = SparkSession.builder().appName("datamodification").master("local").getOrCreate())
        {
        	String tableName = "annual_reports";
            String tempTableName = tableName + "_temp";
        	Properties connectionProperties = new Properties();
            connectionProperties.put("user", mysqlUsername);
            connectionProperties.put("password", mysqlPassword);
            
        	//CSV files to DataFrame
    		Dataset<Row> data = session.read().jdbc(mysqlUrl, tableName, connectionProperties);
    		data.show();
    		Dataset<Row> modifiedDF = data.withColumn("Industry_code_NZSIOC", 
    	            functions.when(data.col("Industry_code_NZSIOC").equalTo(99999), 88888)
    	            .otherwise(data.col("Industry_code_NZSIOC")));
    		
    		modifiedDF.show();
    		//Storing Final DataFrame into MySQL by mentioning the Format, mySql Connection properties, tablename and savemode to save in particular mode
            // Define a temporary table name//
            // Write the renamed DataFrame to a temporary table in MySQL
    		modifiedDF.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);
            // Read back from the temporary table (this ensures the schema is updated in MySQL)
            Dataset<Row> finalDF = session.read()
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);

            // Write the final DataFrame back to the original table, preserving the data
            finalDF.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tableName, connectionProperties);
            session.stop();
            return "success";
        } catch (Exception e) {
            e.printStackTrace();
            return "Spark";
        }
        
    }
    public String migrateSqlToMongo()
    {
    	try(SparkSession session = SparkSession.builder().appName("Migrate Sql to Mongo").master("local").config("spark.mongodb.output.uri", mongodbUri).getOrCreate())
    	{
    		String tableName = "annual_reports";
    		Dataset<Row> sqlData = session.read().format("jdbc").option("url", mysqlUrl).option("dbtable", tableName).option("user", mysqlUsername).option("password", mysqlPassword).option("driver", mysqlDriver).load();
    		
            Dataset<Row> mongodbData = sqlData.withColumn("_id", row_number().over(Window.orderBy(lit(1))));
            
            mongodbData.write().mode(SaveMode.Overwrite)
            .format("mongo")
            .option("uri", mongodbUri)
            .option("database", mongodbname)
            .option("collection", "spark_migration")
            .save();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    
    
    public String migrateMongoToSql()
    {
    	try(SparkSession session = SparkSession.builder().appName("Migrate Mongo to Sql").master("local").getOrCreate())
    	{
    		String tableName = "MigratedData_Mongo";
    		Dataset<Row> mongoData = session.read()
                    .format("mongo")
                    .option("uri", mongodbUri)
                    .option("database", mongodbname)
                    .option("collection", "spark_migration")
                    .load();
    		
    		mongoData.write().format("jdbc").mode(SaveMode.Overwrite)
			.option("url", mysqlUrl)
			.option("dbtable", tableName)
			.option("user", mysqlUsername)
			.option("password", mysqlPassword)
			.save();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    
    
    public String migrateMongoToMongo()
    {
    	try(SparkSession session = SparkSession.builder().appName("Migrate Mongo to Mongo").master("local").getOrCreate())
    	{
    		Dataset<Row> mongoData = session.read()
                    .format("mongo")
                    .option("uri", mongodbUri)
                    .option("database", mongodbname)
                    .option("collection", "spark_migration")
                    .load();
    		
    		mongoData.write().mode(SaveMode.Overwrite)
    		.format("mongo")
			.option("uri", destMongoUri)
			.option("database", destMongodbName)
			.option("collection", "mongotomongo")
			.save();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    
    public String processCsvToMongo(String csvPath)
    {
    	try(SparkSession session = SparkSession.builder().appName("Process CSV to Mongo").master("local").config("spark.mongodb.output.uri", mongodbUri).getOrCreate())
    	{
    		Dataset<Row> csvData = session.read().format("csv").option("header", true).option("inferSchema", true).csv(csvPath);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);

            Dataset<Row> mongodbData = csvData.withColumn("_id", row_number().over(Window.orderBy(lit(1))));
            mongodbData.write().mode(SaveMode.Overwrite) // Choose save mode according to your requirements
            .format("mongo")
            .option("uri", mongodbUri)
            .option("database", mongodbname)
            .option("collection", "spark_migration")
            .save();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    
	public String processCsvToMongo(String csvPath, String csvPath2)
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Process 2 Csv Files to Mongo").master("local").config("spark.mongodb.output.uri", mongodbUri).config("spark.mongodb.output.database", mongodbname).config("spark.mongodb.output.collection", "parent_child_relation").getOrCreate())
    	{
    		//CSV files to DataFrame - 1
    		Dataset<Row> csvData = session.read().format("csv").option("header", "true").option("inferSchema", "true").csv(csvPath);
    		csvData = addIdColumn(csvData);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);
    		
    		//CSV files to DataFrame - 2
    		Dataset<Row> csvData2 = session.read().format("csv").option("header", "true").option("inferSchema", "true").csv(csvPath2);
    		csvData2 = addIdColumn(csvData2);
    		csvData2 = renameColumnsWithSuffix(csvData, csvData2, "_2");
    		csvData2.printSchema();
    		csvData2 = convertStringColumnsToInteger(csvData2);
    		
    		
    		List<String> parentCols = Arrays.asList(csvData.columns());
            JavaRDD<Document> parentRDD = csvData.javaRDD().map(row -> {
                Document doc = new Document();
                for (String col : parentCols) {
                    doc.put(col, row.getAs(col));
                }
                return doc;
            });

            // Convert child DataFrame to RDD of Documents
            List<String> childCols = Arrays.asList(csvData2.columns());
            JavaRDD<Document> childRDD = csvData2.javaRDD().map(row -> {
                Document doc = new Document();
                for (String col : childCols) {
                    doc.put(col, row.getAs(col));
                }
                return doc;
            });

            // Group child documents by id_2 and nest them under their respective parents
            JavaRDD<Document> finalRDD = parentRDD.mapToPair(parentDoc -> {
                Integer id = parentDoc.getInteger("id");
                return new Tuple2<>(id, parentDoc);
            }).leftOuterJoin(childRDD.mapToPair(childDoc -> {
                Integer id2 = childDoc.getInteger("id_2");
                return new Tuple2<>(id2, childDoc);
            }).groupByKey()).map(tuple -> {
                Document parentDoc = tuple._2._1;
                Iterable<Document> children = tuple._2._2.orElse(Collections.emptyList());
                parentDoc.put("children", children);
                return parentDoc;
            });

            // Print the contents of the final RDD to debug
            List<Document> finalList = finalRDD.collect();
            for (Document doc : finalList) {
                System.out.println(doc.toJson());
            }
            
            WriteConfig writeConfig = WriteConfig.create(session).withOption("database", mongodbname).withOption("collection", "parent_child_relation");
            MongoSpark.save(finalRDD, writeConfig);
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}

    }
    private static String getMySQLDataType(String sparkDataType)
	{
		switch (sparkDataType.toLowerCase())
		{
		case "string":
            return "VARCHAR(255)";
        case "integer":
            return "INT";
        case "long":
            return "BIGINT";
        case "double":
            return "DOUBLE";
        case "float":
            return "FLOAT";
        case "boolean":
            return "BOOLEAN";
        case "timestamp":
            return "TIMESTAMP";
        case "date":
            return "DATE";
        default:
            throw new IllegalArgumentException("Unsupported Spark SQL type: " + sparkDataType);
		}
	}
    
	//Method to Add Column to the exiting DataFrame
    private static Dataset<Row> addIdColumn(Dataset<Row> data) 
    {
        // Create a new column 'id' with row number as its value 
        WindowSpec windowSpec = Window.orderBy(functions.lit(1));
        return data.withColumn("id", functions.row_number().over(windowSpec));
    }
    private static String generateCreateTableColumnTypes(StructType schema) 
    {
        return Stream.of(schema.fields())
                .map(field -> field.name().toUpperCase() + " " + getMySQLDataType(field.dataType().typeName()))
                .collect(Collectors.joining(", "));
    }
    
    //Converting String Values to Integer ex:- if values are like 123,456
    private static Dataset<Row> convertStringColumnsToInteger(Dataset<Row> data) 
    {
        StructType schema = data.schema().add("id", "integer");
        for (StructField field : schema.fields()) {
            if (field.dataType() == DataTypes.StringType) {
                // Assuming columns that are strings but contain numbers with commas need to be converted
                String columnName = field.name();
                boolean shouldConvert = data.select(columnName).as(Encoders.STRING()).limit(100)
                        .collectAsList()
                        .stream()
                        .allMatch(value -> value == null || value.matches("^\\d{1,3}(,\\d{3})*(\\.\\d+)?$"));
                if (shouldConvert) {
                	data = data.withColumn(columnName,
                            functions.regexp_replace(functions.col(columnName), ",", "").cast(DataTypes.IntegerType));
                }
            }
        }
        return data;
    }
    
    public String migrateSqlToSql()
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Migrate Sql to Sql").master("local").getOrCreate())
    	{
    		String tableName = "annual_reports";
    		Dataset<Row> data = session.read()
                    .format("jdbc")
                    .option("url", mysqlUrl)
                    .option("dbtable", tableName)
                    .option("user", mysqlUsername)
                    .option("password", mysqlPassword)
                    .load();
    		
    		data.write().format("jdbc").mode(SaveMode.Overwrite)
                    .option("url", destSqlUrl)
                    .option("dbtable", tableName)
                    .option("user", mysqlUsername)
                    .option("password", mysqlPassword)
                    .save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}

    }
    public String processCsvToMySql(String csvPath)
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Csv to Sql").master("local").getOrCreate())
    	{
    		//CSV files to DataFrame
    		Dataset<Row> csvData = session.read().format("csv").option("header", "true").option("inferSchema", "true").csv(csvPath);
    		csvData = addIdColumn(csvData);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);
    		csvData.printSchema();
            String createTableColumnTypes = generateCreateTableColumnTypes(csvData.schema());
    		String tableName = "annual_reports";
    		//Storing Final DataFrame into MySQL by mentioning the Format, mySql Connection properties, tablename and savemode to save in particular mode
    		csvData.write().format("jdbc").mode(SaveMode.Overwrite)
    										.option("url", mysqlUrl)
    										.option("dbtable", tableName)
    										.option("user", mysqlUsername)
    										.option("password", mysqlPassword)
    										.option("createTableColumnTypes", createTableColumnTypes)
    										.save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}

    }
    
    private String schemaToSql(StructType schema) 
    {
        StringBuilder sb = new StringBuilder();
        for (StructField field : schema.fields()) {
            // Append field name and type to the SQL string
            sb.append(field.name()).append(" ").append(getMySQLDataType(field.dataType().typeName())).append(", ");
        }
        // Remove the trailing comma and space
        sb.setLength(sb.length() - 2);
        return sb.toString();
    }
    
    private void createTableIfNotExists(SparkSession session, String keyspace, String table,  StructType schema) 
    {
        String schemaString = schemaToSql(schema);
        String createTableQuery = String.format("CREATE TABLE IF NOT EXISTS %s.%s (%s)", keyspace, table, schemaString);
        session.sql(createTableQuery);
    }
    public String processCsvToCassandra(String csvPath)
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Csv to Cassandra")
    			.config("spark.sql.catalog.mycatalog", "com.datastax.spark.connector.datasource.CassandraCatalog")
    			.config("spark.cassandra.connection.host", "localhost")
    			.config("spark.sql.catalog.mycatalog.spark.cassandra.connection.port", "9042")
    			.config("spark.sql.shuffle.partitions", "1").master("local").getOrCreate())
    	{
    		//CSV files to DataFrame
    		Dataset<Row> csvData = session.read().format("csv").option("header", "true").option("inferSchema", "true").csv(csvPath);
    		csvData = addIdColumn(csvData);
    		csvData = convertStringColumnsToInteger(csvData);
    		String cassandraTable = "annual_reports";
            createTableIfNotExists(session, cassandraKeyspace, cassandraTable, csvData.schema());
            
//    		csvData.write().format("org.apache.spark.sql.cassandra")
//    										.option("keyspace", cassandraKeyspace)
//    										.option("table", cassandraTable)
//    										.mode(SaveMode.Overwrite)
//    										.save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    //Rename Columns For Joining Two Relation Tables
    private static Dataset<Row> renameColumnsWithSuffix(Dataset<Row> csvData, Dataset<Row> csvData2, String suffix) 
    {
        List<String> csvDataColumns = Arrays.asList(csvData.columns()).stream().map(String :: toLowerCase).collect(Collectors.toList());
        System.out.println(csvDataColumns);
        for (String colName : csvData2.columns()) {
            if (csvDataColumns.contains(colName.toLowerCase())) {
            	csvData2 = csvData2.withColumnRenamed(colName, colName + suffix);
            }
        }
        return csvData2;
    }
    
    public String processCsvToMySql(String csvPath, String csvPath2)
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("2 Csv Files to Sql").master("local").getOrCreate())
    	{
    		//CSV files to DataFrame - 1
    		Dataset<Row> csvData = session.read().format("csv").option("header", "true").option("inferSchema", "true").csv(csvPath);
    		csvData = addIdColumn(csvData);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);
    		
    		//CSV files to DataFrame - 2
    		Dataset<Row> csvData2 = session.read().format("csv").option("header", "true").option("inferSchema", "true").csv(csvPath2);
    		csvData2 = addIdColumn(csvData2);
    		csvData2 = renameColumnsWithSuffix(csvData, csvData2, "_2");
    		csvData2.printSchema();
    		csvData2 = convertStringColumnsToInteger(csvData2);
    		
    		//Inner Joining DataFrame 1 and Dataframe 2 by matching id and Creating 3rd DataFrame
            Dataset<Row> joinedDF = csvData.join(csvData2, csvData.col("id").equalTo(csvData2.col("id_2")), "inner");
            joinedDF.show();
            joinedDF.printSchema();
            String createTableColumnTypes = generateCreateTableColumnTypes(joinedDF.schema());
            System.out.println(createTableColumnTypes);
    		String tableName = "joined_annual_reports";
    		
    		//Storing Final DataFrame into MySQL by mentioning the Format, mySql Connection properties, tablename and savemode to save in particular mode
    		joinedDF.write().format("jdbc").mode(SaveMode.Overwrite)
    										.option("url", mysqlUrl)
    										.option("dbtable", tableName)
    										.option("user", mysqlUsername)
    										.option("password", mysqlPassword)
    										.option("createTableColumnTypes", createTableColumnTypes)
    										.save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}

    }
}
