package com.sparkapp.service;

import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.functions.row_number;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.expressions.WindowSpec;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import org.bson.Document;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.stereotype.Service;

import com.datastax.spark.connector.cql.CassandraConnector;
import com.mongodb.spark.MongoSpark;
import com.mongodb.spark.config.WriteConfig;

import scala.Tuple2;



@Service
public class SparkService 
{
	@Value("${spring.datasource.url}")
	private String mysqlUrl;

    @Value("${spring.datasource.username}")
    private String mysqlUsername;

    @Value("${spring.datasource.password}")
    private String mysqlPassword;

    @Value("${spring.datasource.driver-class-name}")
    private String mysqlDriver;
    
    @Value("${spring.data.mongodb.uri}")
    private String mongodbUri;
    
    @Value("${spring.data.mongodb.database}")
    private String mongodbname;
    
    @Value("${spring.data.cassandra.contact-points}")
    private String cassandraHost;

    @Value("${spring.data.cassandra.port}")
    private int cassandraPort;

    @Value("${spring.data.cassandra.keyspace-name}")
    private String cassandraKeyspace;
    
    private String destSqlUrl = "jdbc:mysql://localhost:3306/dkakash";
    
    private String destMongoUri = "mongodb+srv://dkakash0505:AkashExf@sparktests.ommjaa8.mongodb.net/";
    private String destMongodbName = "MongoToMongo";
    	
    public ResponseEntity<?> migrateSqlToMongo()
    {
        long startTime = System.currentTimeMillis();
    	try(SparkSession session = SparkSession.builder().appName("Migrate Sql to Mongo").master("local[*]").getOrCreate())
    	{
    		String tableName = "annual_reports";
    		Dataset<Row> sqlData = session.read()
    				.format("jdbc")
    				.option("url", mysqlUrl)
    				.option("dbtable", tableName)
    				.option("user", mysqlUsername)
    				.option("password", mysqlPassword)
    				.option("driver", mysqlDriver)
    				.load();
    		
            Dataset<Row> modifiedMongoData = sqlData.withColumn("_id", row_number().over(Window.orderBy(lit(1)))).drop("id");
            modifiedMongoData.write().mode(SaveMode.Overwrite)
            .format("mongo")
            .option("uri", mongodbUri)
            .option("database", mongodbname)
            .option("collection", "spark_migration")
            .save();
            
    		List<Map<String, Object>> result = modifiedMongoData.collectAsList().stream().map(row -> {
				Map<String, Object> map = new HashMap<>();
				for (String field : row.schema().fieldNames()) 
				{
					map.put(field, row.getAs(field));
				}
				return map;
			}).collect(Collectors.toList());
    		
            long endTime = System.currentTimeMillis();
            long duration = (endTime - startTime) / 1000; // Convert to seconds
            System.out.println("Execution time in seconds: " + duration);
            return new ResponseEntity<>(result, HttpStatus.OK);
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return new ResponseEntity<>("Exception in Application", HttpStatus.BAD_REQUEST);
    	}
    }
    
    public ResponseEntity<?> migrateMongoToSql()
    {
        long startTime = System.currentTimeMillis();
    	try(SparkSession session = SparkSession.builder().appName("Migrate Mongo to Sql").master("local[*]").getOrCreate())
    	{
    		String tableName = "MigratedData_Mongo";
    		Dataset<Row> mongoData = session.read()
                    .format("mongo")
                    .option("uri", mongodbUri)
                    .option("database", mongodbname)
                    .option("collection", "spark_migration")
                    .load()
                    .drop("_id");
    		
    		String createTableColumnTypes = generateCreateTableColumnTypes(mongoData.schema());
    		mongoData.write().format("jdbc").mode(SaveMode.Overwrite)
			.option("url", mysqlUrl)
			.option("dbtable", tableName)
			.option("user", mysqlUsername)
			.option("password", mysqlPassword)
			.option("createTableColumnTypes", createTableColumnTypes)
			.save();

            List<Map<String, Object>> result = mapDataToList(mongoData);
    		logExecutionTime(startTime);
    		return new ResponseEntity<>(result, HttpStatus.OK);
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return new ResponseEntity<>("Exception in Application", HttpStatus.BAD_REQUEST);
    	}
    }
    
    public ResponseEntity<?> migrateMongoToMongo()
    {
        long startTime = System.currentTimeMillis();
    	try(SparkSession session = SparkSession.builder().appName("Migrate Mongo to Mongo").master("local[*]").getOrCreate())
    	{
    		Dataset<Row> mongoData = session.read()
                    .format("mongo")
                    .option("uri", mongodbUri)
                    .option("database", mongodbname)
                    .option("collection", "spark_migration")
                    .load();
    		
    		mongoData.write().mode(SaveMode.Overwrite)
    		.format("mongo")
			.option("uri", destMongoUri)
			.option("database", destMongodbName)
			.option("collection", "mongotomongo")
			.save();
            List<Map<String, Object>> result = mapDataToList(mongoData);
    		logExecutionTime(startTime);
    		return new ResponseEntity<>(result, HttpStatus.OK);
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return new ResponseEntity<>("Exception in Application", HttpStatus.BAD_REQUEST);
    	}
    }
    
    
    private void logExecutionTime(long startTime) 
    {
        long endTime = System.currentTimeMillis();
        long duration = (endTime - startTime) / 1000; // Convert to seconds
        System.out.println("Execution time in seconds: " + duration);
    }
    
    private List<Map<String, Object>> mapDataToList(Dataset<Row> mongoData) 
    {
        return mongoData.collectAsList().stream()
                .map(row -> {
                    Map<String, Object> map = new HashMap<>();
                    for (String field : row.schema().fieldNames()) {
                        map.put(field, row.getAs(field));
                    }
                    return map;
                }).collect(Collectors.toList());
    }
    
    public ResponseEntity<?> processCsvToMongo(String csvPath)
    {
    	try(SparkSession session = SparkSession.builder().appName("Process CSV to Mongo").master("local").config("spark.mongodb.output.uri", mongodbUri).getOrCreate())
    	{
    		Dataset<Row> csvData = session.read()
    				.format("csv")
    				.option("header", true)
    				.option("inferSchema", true)
    				.csv(csvPath);
    		
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);

            Dataset<Row> mongodbData = csvData.withColumn("_id", row_number().over(Window.orderBy(lit(1))));
            mongodbData.write().mode(SaveMode.Overwrite) // Choose save mode according to your requirements
            .format("mongo")
            .option("uri", mongodbUri)
            .option("database", mongodbname)
            .option("collection", "csv_data")
            .save();
            
    		List<Map<String, Object>> result = mongodbData.collectAsList().stream().map(row -> {
				Map<String, Object> map = new HashMap<>();
				for (String field : row.schema().fieldNames()) 
				{
					map.put(field, row.getAs(field));
				}
				return map;
			}).collect(Collectors.toList());
    		
            return new ResponseEntity<>(result, HttpStatus.OK);
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return new ResponseEntity<>("Exception in Application", HttpStatus.BAD_REQUEST);
    	}
    }
    
    public ResponseEntity<?> processCsvToMongo(String csvPath, String csvPath2)
    {
    	try(SparkSession session = SparkSession.builder().appName("Process 2 Csv Files to Mongo").master("local[*]").config("spark.mongodb.output.uri", mongodbUri).config("spark.mongodb.output.database", mongodbname).config("spark.mongodb.output.collection", "parent_child_relation").getOrCreate())
    	{
    		//CSV files to DataFrame - 1
    		Dataset<Row> csvData = session.read()
    				.format("csv")
    				.option("header", "true")
    				.option("inferSchema", "true")
    				.csv(csvPath);
    		
    		csvData = addIdColumn(csvData);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);
    		
    		//CSV files to DataFrame - 2
    		Dataset<Row> csvData2 = session.read()
    				.format("csv")
    				.option("header", "true")
    				.option("inferSchema", "true")
    				.csv(csvPath2);
    		
    		csvData2 = addIdColumn(csvData2);
    		csvData2 = renameColumnsWithSuffix(csvData, csvData2, "_2");
    		csvData2.printSchema();
    		csvData2 = convertStringColumnsToInteger(csvData2);
    		
    		
    		List<String> parentCols = Arrays.asList(csvData.columns());
            JavaRDD<Document> parentRDD = csvData.javaRDD().map(row -> {
                Document doc = new Document();
                for (String col : parentCols) 
                {
                    doc.put(col, row.getAs(col));
                }
                return doc;
            });

            // Convert child DataFrame to RDD of Documents
            List<String> childCols = Arrays.asList(csvData2.columns());
            JavaRDD<Document> childRDD = csvData2.javaRDD().map(row -> {
                Document doc = new Document();
                for (String col : childCols) 
                {
                    doc.put(col, row.getAs(col));
                }
                return doc;
            });

            // Group child documents by id_2 and nest them under their respective parents
            JavaRDD<Document> finalRDD = parentRDD.mapToPair(parentDoc -> {
                Integer id = parentDoc.getInteger("id");
                return new Tuple2<>(id, parentDoc);
            }).leftOuterJoin(childRDD.mapToPair(childDoc -> {
                Integer id2 = childDoc.getInteger("id_2");
                return new Tuple2<>(id2, childDoc);
            }).groupByKey()).map(tuple -> {
                Document parentDoc = tuple._2._1;
                Iterable<Document> children = tuple._2._2.orElse(Collections.emptyList());
                parentDoc.put("children", children);
                return parentDoc;
            });

            WriteConfig writeConfig = WriteConfig.create(session)
            		.withOption("database", mongodbname)
            		.withOption("collection", "parent_child_relation");
            MongoSpark.save(finalRDD, writeConfig);
            
            List<Map<String, Object>> result = finalRDD.collect().stream().map(row -> {
				Map<String, Object> map = new HashMap<>();
				for (String field : row.keySet()) 
				{
					map.put(field, row.get(field));
				}
				return map;
			}).collect(Collectors.toList());
            session.stop();
       
            return new ResponseEntity<>(result, HttpStatus.OK);
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return new ResponseEntity<>("Exception in Application", HttpStatus.BAD_REQUEST);
    	}

    }
    
    public ResponseEntity<?> processCsvToMySql(String csvPath)
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Csv to Sql").master("local").getOrCreate())
    	{
    		//CSV files to DataFrame
    		Dataset<Row> csvData = session.read()
    				.format("csv")
    				.option("header", "true")
    				.option("inferSchema", "true")
    				.csv(csvPath);
    		
    		csvData = addIdColumn(csvData);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);
    		csvData.printSchema();
    		
            String createTableColumnTypes = generateCreateTableColumnTypes(csvData.schema());
    		String tableName = "annual_reports";
    		
    		//Storing Final DataFrame into MySQL by mentioning the Format, mySql Connection properties, tablename and savemode to save in particular mode
    		csvData.write()
    		.format("jdbc")
    		.mode(SaveMode.Overwrite)
    		.option("url", mysqlUrl)
    		.option("dbtable", tableName)
    		.option("user", mysqlUsername)
    		.option("password", mysqlPassword)
    		.option("createTableColumnTypes", createTableColumnTypes)
    		.save();
    		
    		List<Map<String, Object>> result = csvData.collectAsList().stream().map(row -> {
				Map<String, Object> map = new HashMap<>();
				for (String field : row.schema().fieldNames()) 
				{
					map.put(field, row.getAs(field));
				}
				return map;
			}).collect(Collectors.toList());
            session.stop();
            return new ResponseEntity<>(result, HttpStatus.OK);
            
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return new ResponseEntity<>("Exception in Application", HttpStatus.BAD_REQUEST);  	}
    }
    
    public String renameColumnInMySql()
    {
        try(SparkSession session = SparkSession.builder().appName("colrename").master("local").getOrCreate())
        {
        	String tableName = "annual_reports";
            String tempTableName = tableName + "_temp";

        	Properties connectionProperties = new Properties();
            connectionProperties.put("user", mysqlUsername);
            connectionProperties.put("password", mysqlPassword);
            
        	//CSV files to DataFrame
    		Dataset<Row> data = session.read()
    				.jdbc(mysqlUrl, tableName, connectionProperties);
    		data.show();
    		Dataset<Row> updatedData = data.withColumnRenamed("value", "values");
    		updatedData.show();
    		
            updatedData.write().format("jdbc")
            .mode(SaveMode.Overwrite)
            .option("url", mysqlUrl)
            .option("dbtable", tableName)
            .option("user", mysqlUsername)
			.option("password", mysqlPassword)
			.save();
            

            // Define a temporary table name//
            // Write the renamed DataFrame to a temporary table in MySQL
            updatedData.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);
            // Read back from the temporary table (this ensures the schema is updated in MySQL)
            Dataset<Row> finalDF = session.read()
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);

            // Write the final DataFrame back to the original table, preserving the data
            finalDF.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tableName, connectionProperties);

            session.stop();
            return "success";
        } 
        catch (Exception e)
        {
            e.printStackTrace();
            return "Spark";
        }
        
    }
    
    public String modifyDataInMySql()
    {
        try(SparkSession session = SparkSession.builder().appName("datamodification").master("local").getOrCreate())
        {
        	String tableName = "annual_reports";
            String tempTableName = tableName + "_temp";
        	Properties connectionProperties = new Properties();
            connectionProperties.put("user", mysqlUsername);
            connectionProperties.put("password", mysqlPassword);
            
        	//CSV files to DataFrame
    		Dataset<Row> data = session.read()
    				.jdbc(mysqlUrl, tableName, connectionProperties);
    		data.show();
    		
    		Dataset<Row> modifiedDF = data.withColumn("Industry_code_NZSIOC", functions.when(data.col("Industry_code_NZSIOC").equalTo(99999), 88888).otherwise(data.col("Industry_code_NZSIOC")));
    		
    		modifiedDF.show();
    		//Storing Final DataFrame into MySQL by mentioning the Format, mySql Connection properties, tablename and savemode to save in particular mode
            // Define a temporary table name//
            // Write the renamed DataFrame to a temporary table in MySQL
    		modifiedDF.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);
            // Read back from the temporary table (this ensures the schema is updated in MySQL)
            Dataset<Row> finalDF = session.read()
                    .jdbc(mysqlUrl, tempTableName, connectionProperties);

            // Write the final DataFrame back to the original table, preserving the data
            finalDF.write()
                    .mode(SaveMode.Overwrite)
                    .jdbc(mysqlUrl, tableName, connectionProperties);
            session.stop();
            return "success";
        } 
        catch (Exception e) 
        {
            e.printStackTrace();
            return "Spark";
        }
        
    }
    
    public String modifyDataInMongo()
    {
    	try(SparkSession session = SparkSession.builder().appName("Modify data").master("local").config("spark.cassandra.connection.host", "localhost").config("spark.cassandra.connection.port", "9042").getOrCreate())
    	{
    		Dataset<Row> cassandraData = session.read()
    		        .format("org.apache.spark.sql.cassandra")
    		        .option("keyspace", cassandraKeyspace)
    		        .option("table", "annual_reports")
    		        .load();
    		cassandraData = cassandraData.orderBy("_id");
    		cassandraData.show();
    		
    		Dataset<Row> updatedData = cassandraData.withColumnRenamed("value", "Values");
    		
    		updatedData = updatedData.withColumn("Industry_code_NZSIOC", functions.when(updatedData.col("Industry_code_NZSIOC").equalTo(99999), 88888).otherwise(updatedData.col("Industry_code_NZSIOC")));
    		updatedData.show();
    		updatedData.write()
    		.mode(SaveMode.Overwrite)
    		.format("Mongo")
    		.option("uri", mongodbUri)
    		.option("database", mongodbname)
    		.option("collection", "CassandraMigration")
    		.save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    

    public String migrateCassandraToMongo()
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Csv to Cassandra").master("local").config("spark.cassandra.connection.host", "localhost").config("spark.cassandra.connection.port", "9042").getOrCreate())
    	{
    		Dataset<Row> cassandraData = session.read()
    		        .format("org.apache.spark.sql.cassandra")
    		        .option("keyspace", cassandraKeyspace)
    		        .option("table", "annual_reports")
    		        .load();
    		cassandraData.show();
    		
    		cassandraData = cassandraData.orderBy("_id");
    		
    		cassandraData.show();
    		cassandraData.write()
    		.mode(SaveMode.Overwrite)
    		.format("Mongo")
    		.option("uri", mongodbUri)
    		.option("database", mongodbname)
    		.option("collection", "CassandraMigration")
    		.save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    
    private String generateCreateTableCql(String keyspace, String table, StructType schema)
    {
        StringBuilder sb = new StringBuilder();
        sb.append("(");

        for (StructField field : schema.fields()) 
        {
            sb.append("\"").append(field.name()).append("\" ")
              .append(getCassandraType(field.dataType())).append(", ");
        }

        sb.append("PRIMARY KEY (")
          .append("\"").append("_id").append("\"")
          .append("))");

        return sb.toString();
    }

    private String getCassandraType(DataType dataType) 
    {
        // Map Spark SQL data types to Cassandra data types
        if (dataType instanceof org.apache.spark.sql.types.StringType) 
        {
            return "text";
        } 
        else if (dataType instanceof org.apache.spark.sql.types.IntegerType) 
        {
            return "int";
        } 
        else if (dataType instanceof org.apache.spark.sql.types.DoubleType) 
        {
            return "double";
        } 
        else if (dataType instanceof org.apache.spark.sql.types.DateType) 
        {
            return "date";
        } 
        else 
        {
            // Handle other data types as needed
            return "text";
        }
    }
    
    public String migrateMongoToCassandra()
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Csv to Cassandra").master("local").config("spark.cassandra.connection.host", "cassandra").config("spark.cassandra.connection.port", "9042").getOrCreate())
    	{
    		Dataset<Row> mongoData = session.read()
                    .format("mongo")
                    .option("uri", mongodbUri)
                    .option("database", mongodbname)
                    .option("collection", "spark_migration")
                    .load();
    		
    		mongoData.printSchema();
    		mongoData.show();
    		
    		
    		StructType schema = mongoData.schema();
            String createTableCql = generateCreateTableCql(cassandraKeyspace, "annual_reports", schema);
            CassandraConnector.apply(session.sparkContext()).withSessionDo(sessions ->
                    sessions.execute("CREATE TABLE IF NOT EXISTS " + cassandraKeyspace + ".annual_reports " + createTableCql));
            
            
            mongoData.write()
            .format("org.apache.spark.sql.cassandra")
            .option("keyspace", cassandraKeyspace)
            .option("table", "annual_reports")
            .option("batch_size", "1000")
            .mode(SaveMode.Overwrite)
            .option("confirm.truncate", "true")
            .option("spark.cassandra.output.consistency.level", "ONE")
            .save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}
    }
    

    
	
	
    private static String getMySQLDataType(String sparkDataType)
	{
		switch (sparkDataType.toLowerCase())
		{
		case "string":
            return "VARCHAR(255)";
        case "integer":
            return "INT";
        case "long":
            return "BIGINT";
        case "double":
            return "DOUBLE";
        case "float":
            return "FLOAT";
        case "boolean":
            return "BOOLEAN";
        case "timestamp":
            return "TIMESTAMP";
        case "date":
            return "DATE";
        default:
            throw new IllegalArgumentException("Unsupported Spark SQL type: " + sparkDataType);
		}
	}
    
	//Method to Add Column to the exiting DataFrame
    private static Dataset<Row> addIdColumn(Dataset<Row> data) 
    {
        // Create a new column 'id' with row number as its value 
        WindowSpec windowSpec = Window.orderBy(functions.lit(1));
        return data.withColumn("id", functions.row_number().over(windowSpec));
    }
    
    private static String generateCreateTableColumnTypes(StructType schema) 
    {
        return Stream.of(schema.fields())
                .map(field -> field.name().toUpperCase() + " " + getMySQLDataType(field.dataType().typeName()))
                .collect(Collectors.joining(", "));
    }
    
    //Converting String Values to Integer ex:- if values are like 123,456
    private static Dataset<Row> convertStringColumnsToInteger(Dataset<Row> data) 
    {
        StructType schema = data.schema().add("id", "integer");
        for (StructField field : schema.fields()) {
            if (field.dataType() == DataTypes.StringType) {
                // Assuming columns that are strings but contain numbers with commas need to be converted
                String columnName = field.name();
                boolean shouldConvert = data.select(columnName)
                		.as(Encoders.STRING()).limit(100)
                        .collectAsList()
                        .stream()
                        .allMatch(value -> value == null || value.matches("^\\d{1,3}(,\\d{3})*(\\.\\d+)?$"));
                if (shouldConvert) 
                {
                	data = data.withColumn(columnName,
                            functions.regexp_replace(functions.col(columnName), ",", "").cast(DataTypes.IntegerType));
                }
            }
        }
        return data;
    }
    
    public String migrateSqlToSql()
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("Migrate Sql to Sql").master("local").getOrCreate())
    	{
    		String tableName = "annual_reports";
    		Dataset<Row> data = session.read()
                    .format("jdbc")
                    .option("url", mysqlUrl)
                    .option("dbtable", tableName)
                    .option("user", mysqlUsername)
                    .option("password", mysqlPassword)
                    .load();
    		
    		data.write().format("jdbc").mode(SaveMode.Overwrite)
                    .option("url", destSqlUrl)
                    .option("dbtable", tableName)
                    .option("user", mysqlUsername)
                    .option("password", mysqlPassword)
                    .save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}

    }
       
    //Rename Columns For Joining Two Relation Tables
    private static Dataset<Row> renameColumnsWithSuffix(Dataset<Row> csvData, Dataset<Row> csvData2, String suffix) 
    {
        List<String> csvDataColumns = Arrays.asList(csvData.columns()).stream().map(String :: toLowerCase).collect(Collectors.toList());
        System.out.println(csvDataColumns);
        for (String colName : csvData2.columns()) 
        {
            if (csvDataColumns.contains(colName.toLowerCase())) 
            {
            	csvData2 = csvData2.withColumnRenamed(colName, colName + suffix);
            }
        }
        return csvData2;
    }
    
    public String processCsvToMySql(String csvPath, String csvPath2)
    {
    	//Create Spark Session and handle exceptions
    	try(SparkSession session = SparkSession.builder().appName("2 Csv Files to Sql").master("local").getOrCreate())
    	{
    		//CSV files to DataFrame - 1
    		Dataset<Row> csvData = session.read()
    				.format("csv")
    				.option("header", "true")
    				.option("inferSchema", "true")
    				.csv(csvPath);
    		
    		csvData = addIdColumn(csvData);
    		csvData.printSchema();
    		csvData = convertStringColumnsToInteger(csvData);
    		
    		//CSV files to DataFrame - 2
    		Dataset<Row> csvData2 = session.read()
    				.format("csv")
    				.option("header", "true")
    				.option("inferSchema", "true")
    				.csv(csvPath2);
    		
    		csvData2 = addIdColumn(csvData2);
    		csvData2 = renameColumnsWithSuffix(csvData, csvData2, "_2");
    		csvData2.printSchema();
    		csvData2 = convertStringColumnsToInteger(csvData2);
    		
    		//Inner Joining DataFrame 1 and Dataframe 2 by matching id and Creating 3rd DataFrame
            Dataset<Row> joinedDF = csvData.join(csvData2, csvData.col("id").equalTo(csvData2.col("id_2")), "inner");
            joinedDF.show();
            joinedDF.printSchema();
            String createTableColumnTypes = generateCreateTableColumnTypes(joinedDF.schema());
            System.out.println(createTableColumnTypes);
    		String tableName = "joined_annual_reports";
    		
    		//Storing Final DataFrame into MySQL by mentioning the Format, mySql Connection properties, tablename and savemode to save in particular mode
    		joinedDF.write()
    		.format("jdbc")
    		.mode(SaveMode.Overwrite).option("url", mysqlUrl)
    		.option("dbtable", tableName)
    		.option("user", mysqlUsername)
    		.option("password", mysqlPassword)
    		.option("createTableColumnTypes", createTableColumnTypes)
    		.save();
            session.stop();
            return "success";
    	}
        catch(Exception e)
    	{
        	e.printStackTrace();
        	return "Spark";
    	}

    }
}
